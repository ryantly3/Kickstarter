{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "# Kickstarter: Predicting the success/failure of crowdfunded projects\n",
    "Ryan Ly\n",
    "*****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "For the first part of this project, the data is imported, concatenated into one dataframe (because the scraped data consists of 56 separate csv files), and and cleaned in preparation for exploratory data analysis and modeling. Because the data requires a significant amount of cleaning, it was easier to process it using Pandas first and then used the cleaned dataset to train the machine learning models using sklearn and pyspark later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50) # Display up to 50 columns at a time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12,5\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "import glob # To read all csv files in the directory\n",
    "\n",
    "import calendar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n",
    "import itertools\n",
    "import time\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all Kickstarter csvs\n",
    "df = pd.concat([pd.read_csv(f) for f in glob.glob('data/Kickstarter*.csv')], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of projects\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the columns (features)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column descriptions:\n",
    "- backers_count = number of people who contributed funds to the project\n",
    "- blurb = short description of the project\n",
    "- category = contains the category and sub-category of the project\n",
    "- converted_pledged_amount = amount of money pledged, converted to the currency in the 'current_currency' column\n",
    "- country = country the project creator is from\n",
    "- created_at = date and time of when the project was initially created on Kickstarter\n",
    "- creator = name of the project creator and other information about them, e.g. Kickstarter id number\n",
    "- currency = original currency the project goal was denominated in\n",
    "- currency_symbol = symbol of the original currency the project goal was denominated in\n",
    "- currency_trailing_code = code of the original currency the project goal was denominated in\n",
    "- current_currency = currency the project goal was converted to\n",
    "- deadline = date and time of when the project will close for donations\n",
    "- disable_communication = whether or not a project owner disabled communication with their backers\n",
    "- friends = unclear (null or empty)\n",
    "- fx_rate = foreign exchange rate between the original currency and the current_currency\n",
    "- goal = funding goal\n",
    "- id = id number of the project\n",
    "- is_backing = unclear (null or false)\n",
    "- is_starrable = whether or not a project can be starred (liked and saved) by users\n",
    "- is_starred = whether or not a project has been starred (liked and saved) by users\n",
    "- launched_at = date and time of when the project was launched for funding\n",
    "- location = contains the town or city of the project creator\n",
    "- name = name of the project\n",
    "- permissions = unclear (null or empty)\n",
    "- photo = contains a link and information to the project's photo/s\n",
    "- pledged = amount pledged in the current_currency\n",
    "- profile = details about the project's profile, including id number and various visual settings\n",
    "- slug = name of the project with hyphens instead of spaces\n",
    "- source_url = url for the project's category\n",
    "- spotlight = after a project has been successful, it is spotlighted on the Kickstarter website\n",
    "- staff_pick = whether a project was highlighted as a staff_pick when it was launched/live\n",
    "- state = whether a project was successful, failed, canceled, suspending or still live\n",
    "- state_changed_at = date and time of when a project's status was changed (same as the deadline for successful and failed projects)\n",
    "- static_usd_rate = conversion rate between the original currency and USD\n",
    "- urls = url to the project's page\n",
    "- usd_pledged = amount pledged in USD\n",
    "- usd_type = domestic or international"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for number of duplicates of individual projects (will be addressed later)\n",
    "len(df[df.duplicated(subset='id')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking column information\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that are mostly null\n",
    "df.drop(['friends', 'is_backing', 'is_starred', 'permissions'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns that arem't useful:\n",
    "\n",
    "- converted_pledged_amount = most currencies are converted into USD in this column, but not all. Instead, the 'usd_pledged' column will be used as these all use the same currency (the dollar).\n",
    "- creator = most projects are by different people, and so this cannot be usefully used to group or categorise projects, and is not useful in a machine learning context.\n",
    "- currency = all currency values will be used as/converted to dollars, so that they can be evaluated together. It is not necessary to keep the original record because of this, and because it will be highly correlated with country (which will be kept).\n",
    "- currency_symbol = same as above.\n",
    "- currency_trailing_code = same as above.\n",
    "- current_currency = same as above.\n",
    "- fx_rate = this is used to create 'converted_pledged_amount' from 'pledged', but does not always convert to dollars so can be dropped in favour of 'static_usd_rate' which always converts to dollars.\n",
    "- photo = image processing/computer vision will not be used in this project.\n",
    "- pledged = data in this column is stored in native currencies, so this will be dropped in favour of 'usd_pledged' which is all in the same currency (dollars).\n",
    "- profile = this column contains a combination of information from other columns (e.g. id, state, dates, url).\n",
    "- slug = this is simply the 'name' column with hyphens instead of spaces.\n",
    "- source_url = the sites that the rows were each scraped from is not useful for building a model, as each is unique to an id.\n",
    "- spotlight = projects can only be spotlighted after they are already successful, so this will be entirely correlated with successful projects.\n",
    "- state_changed_at = this is the same as deadline for most projects. The only exceptions are for projects which were cancelled before their deadline, but they will not be included in this analysis.\n",
    "- urls = as with source_url.\n",
    "- usd_type = it is unclear what this column means, but it is unlikely to be necessary since all currency values will be converted to dollars, and other currency information has been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping columns that aren't useful\n",
    "df.drop(['converted_pledged_amount', 'creator', 'currency', 'currency_symbol', 'currency_trailing_code', 'current_currency', 'fx_rate', 'photo', 'pledged', 'profile', 'slug', 'source_url', 'spotlight', 'state_changed_at', 'urls', 'usd_type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting dates from unix to datetime\n",
    "cols_to_convert = ['created_at', 'deadline', 'launched_at']\n",
    "for c in cols_to_convert:\n",
    "    df[c] = pd.to_datetime(df[c], origin='unix', unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Earliest listed project date\n",
    "min(df.created_at).strftime('%d %B %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest listed project date\n",
    "max(df.created_at).strftime('%d %B %Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, natural language processing will be deferred unless time permits. However, the length of the blurbs and names will be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count length of each blurb\n",
    "df['blurb_length'] = df['blurb'].str.split().str.len()\n",
    "\n",
    "# Drop blurb variable\n",
    "df.drop('blurb', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View category syntax\n",
    "df.iloc[0]['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extracting the relevant sub-category section from the string\n",
    "f = lambda x: x['category'].split('/')[1].split('\",\"position')[0]\n",
    "df['sub_category'] = df.apply(f, axis=1)\n",
    "\n",
    "# Extracting the relevant category section from the string, and replacing the original category variable\n",
    "f = lambda x: x['category'].split('\"slug\":\"')[1].split('/')[0]\n",
    "df['category'] = df.apply(f, axis=1)\n",
    "f = lambda x: x['category'].split('\",\"position\"')[0] # Some categories do not have a sub-category, so do not have a '/' to split with\n",
    "df['category'] = df.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Counting the number of unique categories\n",
    "df.category.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of unique sub categories\n",
    "df.sub_category.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the proportions of each category\n",
    "df.disable_communication.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop disable communication column (because 99.7% are false)\n",
    "df.drop('disable_communication', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate new column 'usd_goal' as goal * static_usd_rate\n",
    "df['usd_goal'] = round(df['goal'] * df['static_usd_rate'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping goal and static_usd_rate\n",
    "df.drop(['goal', 'static_usd_rate'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out what this is, and do a count_values() to figure out whether it's worth including or mostly FALSE\n",
    "df.is_starrable.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View location syntax\n",
    "df.iloc[0]['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of unique locations\n",
    "df.location.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping location (too many unique locations)\n",
    "df.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count length of each name\n",
    "df['name_length'] = df['name'].str.split().str.len()\n",
    "# Drop name variable\n",
    "df.drop('name', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['usd_pledged'] = round(df['usd_pledged'],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "After dropping columns that were irrevelent and/or not useful for this project, additional features are engineered from existing features:\n",
    "- time from creation to launch\n",
    "- campaign length\n",
    "- launch day of week\n",
    "- deadline day of week, launch month\n",
    "- deadline month\n",
    "- launch time of day\n",
    "- deadline time of day\n",
    "- mean pledge per backer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time between creating and launching a project\n",
    "df['creation_to_launch_days'] = df['launched_at'] - df['created_at']\n",
    "df['creation_to_launch_days'] = df['creation_to_launch_days'].dt.round('d').dt.days # Rounding to nearest days, then showing as number only\n",
    "# Or could show as number of hours:\n",
    "# df['creation_to_launch_hours'] = df['launched_at'] - df['created_at']\n",
    "# df['creation_to_launch_hours'] = df['creation_to_launch_hours'].dt.round('h') / np.timedelta64(1, 'h') \n",
    "\n",
    "# Campaign length\n",
    "df['campaign_days'] = df['deadline'] - df['launched_at']\n",
    "df['campaign_days'] = df['campaign_days'].dt.round('d').dt.days # Rounding to nearest days, then showing as number only\n",
    "\n",
    "# Launch day of week\n",
    "df['launch_day'] = df['launched_at'].dt.weekday_name\n",
    "\n",
    "# Deadline day of week\n",
    "df['deadline_day'] = df['deadline'].dt.weekday_name\n",
    "\n",
    "# Launch month\n",
    "df['launch_month'] = df['launched_at'].dt.month_name()\n",
    "\n",
    "# Deadline month\n",
    "df['deadline_month'] = df['deadline'].dt.month_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Launch time\n",
    "df['launch_hour'] = df['launched_at'].dt.hour # Extracting hour from launched_at\n",
    "\n",
    "def two_hour_launch(row):\n",
    "    '''Creates two hour bins from the launch_hour column'''\n",
    "    if row['launch_hour'] in (0,1):\n",
    "        return '12am-2am'\n",
    "    if row['launch_hour'] in (2,3):\n",
    "        return '2am-4am'\n",
    "    if row['launch_hour'] in (4,5):\n",
    "        return '4am-6am'\n",
    "    if row['launch_hour'] in (6,7):\n",
    "        return '6am-8am'\n",
    "    if row['launch_hour'] in (8,9):\n",
    "        return '8am-10am'\n",
    "    if row['launch_hour'] in (10,11):\n",
    "        return '10am-12pm'\n",
    "    if row['launch_hour'] in (12,13):\n",
    "        return '12pm-2pm'\n",
    "    if row['launch_hour'] in (14,15):\n",
    "        return '2pm-4pm'\n",
    "    if row['launch_hour'] in (16,17):\n",
    "        return '4pm-6pm'\n",
    "    if row['launch_hour'] in (18,19):\n",
    "        return '6pm-8pm'\n",
    "    if row['launch_hour'] in (20,21):\n",
    "        return '8pm-10pm'\n",
    "    if row['launch_hour'] in (22,23):\n",
    "        return '10pm-12am'\n",
    "    \n",
    "df['launch_time'] = df.apply(two_hour_launch, axis=1) # Calculates bins from launch_time\n",
    "\n",
    "df.drop('launch_hour', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deadline time\n",
    "df['deadline_hour'] = df['deadline'].dt.hour # Extracting hour from deadline\n",
    "\n",
    "def two_hour_deadline(row):\n",
    "    '''Creates two hour bins from the deadline_hour column'''\n",
    "    if row['deadline_hour'] in (0,1):\n",
    "        return '12am-2am'\n",
    "    if row['deadline_hour'] in (2,3):\n",
    "        return '2am-4am'\n",
    "    if row['deadline_hour'] in (4,5):\n",
    "        return '4am-6am'\n",
    "    if row['deadline_hour'] in (6,7):\n",
    "        return '6am-8am'\n",
    "    if row['deadline_hour'] in (8,9):\n",
    "        return '8am-10am'\n",
    "    if row['deadline_hour'] in (10,11):\n",
    "        return '10am-12pm'\n",
    "    if row['deadline_hour'] in (12,13):\n",
    "        return '12pm-2pm'\n",
    "    if row['deadline_hour'] in (14,15):\n",
    "        return '2pm-4pm'\n",
    "    if row['deadline_hour'] in (16,17):\n",
    "        return '4pm-6pm'\n",
    "    if row['deadline_hour'] in (18,19):\n",
    "        return '6pm-8pm'\n",
    "    if row['deadline_hour'] in (20,21):\n",
    "        return '8pm-10pm'\n",
    "    if row['deadline_hour'] in (22,23):\n",
    "        return '10pm-12am'\n",
    "    \n",
    "df['deadline_time'] = df.apply(two_hour_deadline, axis=1) # Calculates bins from launch_time\n",
    "\n",
    "df.drop('deadline_hour', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pledge per backer\n",
    "df['pledge_per_backer'] = round(df['usd_pledged']/df['backers_count'],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing null values for blurb_length with 0\n",
    "df.blurb_length.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming there are no null values remaining\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of projects of different states\n",
    "df.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping projects which are not successes or failures\n",
    "df = df[df['state'].isin(['successful', 'failed'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confirming that the most recent deadline is the day on which the data was scraped, i.e. there are no projects which have yet to be resolved into either successes or failures\n",
    "max(df.deadline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking for duplicates of individual projects, and sorting by id\n",
    "duplicates = df[df.duplicated(subset='id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicates which have every value in common\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View leftover duplicate rows\n",
    "duplicated = df[df.duplicated(subset='id', keep=False)].sort_values(by='id')\n",
    "duplicated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get list of index numbers for duplicated ids\n",
    "dup_ids = duplicated.id.unique()\n",
    "for i in dup_ids:\n",
    "    index1 = duplicated[duplicated.id == i][:1].index.values\n",
    "    index2 = duplicated[duplicated.id == i][1:2].index.values\n",
    "    print(index1, index2)\n",
    "    #print(duplicated.loc[index1] == duplicated.loc[index2]) # produces TypeError: Could not compare [None] with block values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[31239] == df.loc[66149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[31239,['usd_goal','usd_pledge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[66149,['usd_goal','usd_pledge']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each pair of duplicates, there are small differences in the usd_pledge and usd_goal columns on the order of a few cents or dollars. It was decided to keep only the first of each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='id', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the id column as the index\n",
    "df.set_index('id', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "In this section, the data is explored and various plots are generated to understand general patterns within the data. Several important features are visualized in more detail, which can provide insight into what might impact the success of a project the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for the numerical features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the average amount pledged to successful and unsuccesful projects\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9)) = plt.subplots(3, 3, figsize=(12,12))\n",
    "\n",
    "df['state'].value_counts(ascending=True).plot(kind='bar', ax=ax1, rot=0)\n",
    "ax1.set_title('Number of projects')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "df.groupby('state').usd_goal.median().plot(kind='bar', ax=ax2, rot=0)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "df.groupby('state').usd_pledged.median().plot(kind='bar', ax=ax3, rot=0)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "df.groupby('state').backers_count.median().plot(kind='bar', ax=ax4, rot=0)\n",
    "ax4.set_title('Median backers per project')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "df.groupby('state').campaign_days.mean().plot(kind='bar', ax=ax5, rot=0)\n",
    "ax5.set_title('Mean campaign length (days)')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "df.groupby('state').creation_to_launch_days.mean().plot(kind='bar', ax=ax6, rot=0)\n",
    "ax6.set_title('Mean creation to launch length (days)')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "df.groupby('state').name_length.mean().plot(kind='bar', ax=ax7, rot=0)\n",
    "ax7.set_title('Mean name length (words)')\n",
    "ax7.set_xlabel('')\n",
    "\n",
    "df.groupby('state').blurb_length.mean().plot(kind='bar', ax=ax8, rot=0)\n",
    "ax8.set_title('Mean blurb length (words)')\n",
    "ax8.set_xlabel('')\n",
    "\n",
    "# Creating a dataframe grouped by staff_pick with columns for failed and successful\n",
    "pick_df = pd.get_dummies(df.set_index('staff_pick').state).groupby('staff_pick').sum()\n",
    "# Normalizes counts by column, and selects the 'True' category (iloc[1])\n",
    "(pick_df.div(pick_df.sum(axis=0), axis=1)).iloc[1].plot(kind='bar', ax=ax9, rot=0) \n",
    "ax9.set_title('Proportion that are staff picks')\n",
    "ax9.set_xlabel('')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graphs, successful projects tend to have a smaller project goal, shorter campaign length, and longer creation to launch length. A high amount pledged, high number of backers, and staff picks are generally expected for successful projects and can be understood more as correlation than causation. The name length and blurb length are about the same for both successful and failed projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the number of projects launched each month\n",
    "plt.figure(figsize=(16,6))\n",
    "df.set_index('launched_at').category.resample('MS').count().plot()\n",
    "plt.xlim('2009-01-01', '2019-02-28') # Limiting to whole months\n",
    "plt.xlabel('Launch date', fontsize=12)\n",
    "plt.ylabel('Number of projects', fontsize=12)\n",
    "plt.title('Number of projects launched on Kickstarter, 2009-2019', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the cumulative amount pledged on Kickstarter\n",
    "plt.figure(figsize=(16,6))\n",
    "df.set_index('launched_at').sort_index().usd_pledged.cumsum().plot()\n",
    "plt.xlim('2009-01-01', '2019-02-28') # Limiting to whole months\n",
    "plt.xlabel('Launch date', fontsize=12)\n",
    "plt.ylabel('Cumulative amount pledged ($)', fontsize=12)\n",
    "plt.title('Cumulative pledges on Kickstarter, 2009-2019', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average amount pledged per project in each year, in $:\")\n",
    "print(round(df.set_index('launched_at').usd_pledged.resample('YS').mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the distribution of pledged amounts each year\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(df.launched_at.dt.year, np.log(df.usd_pledged))\n",
    "plt.xlabel('Year of launch', fontsize=12)\n",
    "plt.ylabel('Amount pledged (log-transformed $)', fontsize=12) # Log-transforming to make the trend clearer, as the distribution is heavily positively skewed\n",
    "plt.title('Amount pledged on Kickstarter projects, 2009-2019', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average fundraising goal per project in each year, in $:\")\n",
    "print(round(df.set_index('launched_at').usd_goal.resample('YS').mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of goal amounts each year\n",
    "plt.figure(figsize=(16,6))\n",
    "sns.boxplot(df.launched_at.dt.year, np.log(df.usd_goal))\n",
    "plt.xlabel('Year of launch', fontsize=12)\n",
    "plt.ylabel('Goal (log-transformed $)', fontsize=12) # Log-transforming to make the trend clearer, as the distribution is heavily positively skewed\n",
    "plt.title('Fundraising goals of Kickstarter projects, 2009-2019', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Significantly more projects are launched after 2014 than in prior years, which might imply a better chance of having a successful project if it was launched in these later years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by year with columns for failed and successful\n",
    "year_df = df.set_index('launched_at').state\n",
    "year_df = pd.get_dummies(year_df).resample('YS').sum()\n",
    "\n",
    "# Plotting the number and proportion of failed and successful projects each year\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,4))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.launch_day.nunique()))\n",
    "\n",
    "year_df.plot.bar(ax=ax[0], color=color)\n",
    "ax[0].set_title('Number of failed and successful projects')\n",
    "ax[0].set_xlabel('')\n",
    "ax[0].set_xticklabels(list(range(2009,2020)), rotation=45)\n",
    "\n",
    "year_df.div(year_df.sum(axis=1), axis=0).successful.plot(kind='bar', ax=ax[1], color=color) # Normalizes counts across rows\n",
    "ax[1].set_title('Proportion of successful projects')\n",
    "ax[1].set_xlabel('')\n",
    "ax[1].set_xticklabels(list(range(2009,2020)), rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, while there are a greater number of succesful projects in 2014 and later, the proportion of successful projects is significantly lower than in the years prior. This may be attributed to the large volume of projects and subsequent competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by category with columns for failed and successful\n",
    "cat_df = pd.get_dummies(df.set_index('category').state).groupby('category').sum()\n",
    "\n",
    "# Plotting\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(12,12))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.category.nunique())) # Setting a colormap\n",
    "\n",
    "df.groupby('category').category.count().plot(kind='bar', ax=ax1, color=color)\n",
    "ax1.set_title('Number of projects')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "df.groupby('category').usd_goal.median().plot(kind='bar', ax=ax2, color=color)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "df.groupby('category').usd_pledged.median().plot(kind='bar', ax=ax3, color=color)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "cat_df.div(cat_df.sum(axis=1), axis=0).successful.plot(kind='bar', ax=ax4, color=color) # Normalizes counts across rows\n",
    "ax4.set_title('Proportion of successful projects')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "df.groupby('category').backers_count.median().plot(kind='bar', ax=ax5, color=color)\n",
    "ax5.set_title('Median backers per project')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "df.groupby('category').pledge_per_backer.median().plot(kind='bar', ax=ax6, color=color)\n",
    "ax6.set_title('Median pledged per backer ($)')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the projects in the most common categories, it seems that projects in the comics and dance category have the largest proportion of successful projects (and highest amount pledged), but also have smaller project goals. Comics and games projects have the most backers, but dance and film & video projects have the most pledged per backer. Technology projects have the highest project goal, but relatively low amount pledged and low number of backers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by country with columns for failed and successful\n",
    "country_df = pd.get_dummies(df.set_index('country').state).groupby('country').sum()\n",
    "\n",
    "# Plotting\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(16,12))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.country.nunique()))\n",
    "\n",
    "df.groupby('country').country.count().plot(kind='bar', ax=ax1, color=color, rot=0)\n",
    "ax1.set_title('Number of projects')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "df.groupby('country').usd_goal.median().plot(kind='bar', ax=ax2, color=color, rot=0)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "df.groupby('country').usd_pledged.median().plot(kind='bar', ax=ax3, color=color, rot=0)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "country_df.div(country_df.sum(axis=1), axis=0).successful.plot(kind='bar', ax=ax4, color=color, rot=0) # Normalizes counts across rows\n",
    "ax4.set_title('Proportion of successful projects')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "df.groupby('country').backers_count.median().plot(kind='bar', ax=ax5, color=color, rot=0)\n",
    "ax5.set_title('Median backers per project')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "df.groupby('country').pledge_per_backer.median().plot(kind='bar', ax=ax6, color=color, rot=0)\n",
    "ax6.set_title('Median pledged per backer ($)')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most projects originate in the United States, Hong Kong has the largest proportion of successful projects, also having the highest median pledged per project and highest number of backers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by the day on which they were launched, with columns for failed and successful\n",
    "day_df = pd.get_dummies(df.set_index('launch_day').state).groupby('launch_day').sum()\n",
    "\n",
    "# Plotting\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(14,12))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.launch_day.nunique()))\n",
    "\n",
    "weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "df.groupby('launch_day').launch_day.count().reindex(weekdays).plot(kind='bar', ax=ax1, color=color, rot=0)\n",
    "ax1.set_title('Number of projects launched')\n",
    "ax1.set_xlabel('')\n",
    "\n",
    "df.groupby('launch_day').usd_goal.median().reindex(weekdays).plot(kind='bar', ax=ax2, color=color, rot=0)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "\n",
    "df.groupby('launch_day').usd_pledged.median().reindex(weekdays).plot(kind='bar', ax=ax3, color=color, rot=0)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "\n",
    "day_df.div(day_df.sum(axis=1), axis=0).successful.reindex(weekdays).plot(kind='bar', ax=ax4, color=color, rot=0) # Normalizes counts across rows\n",
    "ax4.set_title('Proportion of successful projects')\n",
    "ax4.set_xlabel('')\n",
    "\n",
    "df.groupby('launch_day').backers_count.median().reindex(weekdays).plot(kind='bar', ax=ax5, color=color, rot=0)\n",
    "ax5.set_title('Median backers per project')\n",
    "ax5.set_xlabel('')\n",
    "\n",
    "df.groupby('launch_day').pledge_per_backer.median().reindex(weekdays).plot(kind='bar', ax=ax6, color=color, rot=0)\n",
    "ax6.set_title('Median pledged per backer ($)')\n",
    "ax6.set_xlabel('')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most successful projects are launched on a Tuesday with a higher amount pledged and higher number of backers than other days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by the month in which they were launched, with columns for failed and successful\n",
    "month_df = pd.get_dummies(df.set_index('launch_month').state).groupby('launch_month').sum()\n",
    "\n",
    "# Plotting\n",
    "months = list(calendar.month_name)[1:]\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(14,12))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.launch_month.nunique()))\n",
    "\n",
    "df.groupby('launch_month').launch_month.count().reindex(months).plot(kind='bar', ax=ax1, color=color, rot=45)\n",
    "ax1.set_title('Number of projects launched')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticklabels(labels=ax1.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_month').usd_goal.median().reindex(months).plot(kind='bar', ax=ax2, color=color, rot=45)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_xticklabels(labels=ax2.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_month').usd_pledged.median().reindex(months).plot(kind='bar', ax=ax3, color=color, rot=45)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "ax3.set_xticklabels(labels=ax3.get_xticklabels(), ha='right')\n",
    "\n",
    "month_df.div(month_df.sum(axis=1), axis=0).successful.reindex(months).plot(kind='bar', ax=ax4, color=color, rot=45) # Normalizes counts across rows\n",
    "ax4.set_title('Proportion of successful projects')\n",
    "ax4.set_xlabel('')\n",
    "ax4.set_xticklabels(labels=ax4.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_month').backers_count.median().reindex(months).plot(kind='bar', ax=ax5, color=color, rot=45)\n",
    "ax5.set_title('Median backers per project')\n",
    "ax5.set_xlabel('')\n",
    "ax5.set_xticklabels(labels=ax5.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_month').pledge_per_backer.median().reindex(months).plot(kind='bar', ax=ax6, color=color, rot=45)\n",
    "ax6.set_title('Median pledged per backer ($)')\n",
    "ax6.set_xlabel('')\n",
    "ax6.set_xticklabels(labels=ax6.get_xticklabels(), ha='right')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "July is the most popular month to launch a project, but it also has the lowest proportion of successful projects, lowest amount pledged, and lowest number of backers. Otherwise, there are not many clear patterns looking at the time of year of projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe grouped by the time at which they were launched, with columns for failed and successful\n",
    "time_df = pd.get_dummies(df.set_index('launch_time').state).groupby('launch_time').sum()\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = plt.subplots(3, 2, figsize=(14,12))\n",
    "\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.launch_time.nunique()))\n",
    "\n",
    "times = ['12am-2am', '2am-4am', '4am-6am', '6am-8am', '8am-10am', '10am-12pm', '12pm-2pm', '2pm-4pm', '4pm-6pm', '6pm-8pm', '8pm-10pm', '10pm-12am']\n",
    "\n",
    "df.groupby('launch_time').launch_time.count().reindex(times).plot(kind='bar', ax=ax1, color=color, rot=45)\n",
    "ax1.set_title('Number of projects launched')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticklabels(labels=ax1.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_time').usd_goal.median().reindex(times).plot(kind='bar', ax=ax2, color=color, rot=45)\n",
    "ax2.set_title('Median project goal ($)')\n",
    "ax2.set_xlabel('')\n",
    "ax2.set_xticklabels(labels=ax2.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_time').usd_pledged.median().reindex(times).plot(kind='bar', ax=ax3, color=color, rot=45)\n",
    "ax3.set_title('Median pledged per project ($)')\n",
    "ax3.set_xlabel('')\n",
    "ax3.set_xticklabels(labels=ax3.get_xticklabels(), ha='right')\n",
    "\n",
    "time_df.div(time_df.sum(axis=1), axis=0).successful.reindex(times).plot(kind='bar', ax=ax4, color=color, rot=45) # Normalizes counts across rows\n",
    "ax4.set_title('Proportion of successful projects')\n",
    "ax4.set_xlabel('')\n",
    "ax4.set_xticklabels(labels=ax4.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_time').backers_count.median().reindex(times).plot(kind='bar', ax=ax5, color=color, rot=45)\n",
    "ax5.set_title('Median backers per project')\n",
    "ax5.set_xlabel('')\n",
    "ax5.set_xticklabels(labels=ax5.get_xticklabels(), ha='right')\n",
    "\n",
    "df.groupby('launch_time').pledge_per_backer.median().reindex(times).plot(kind='bar', ax=ax6, color=color, rot=45)\n",
    "ax6.set_title('Median pledged per backer ($)')\n",
    "ax6.set_xlabel('')\n",
    "ax6.set_xticklabels(labels=ax6.get_xticklabels(), ha='right')\n",
    "\n",
    "fig.subplots_adjust(hspace=0.45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at project launch times (in UTC/GMT), it seems that between 12pm and 2pm UTC is the best time to launch since it has the highest proportion of successful projects, highest amount pledged, and highest number of backers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********************\n",
    "# Pandas & Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "Finally, we train several different machine learning models to classify projects as successful or unsuccessful. Three different algorithms are used: logistic regression, random forest, and gradient boosting. The performance of these models using Scikit-Learn is compared to the performance of the same models implemented in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping columns and creating new dataframe\n",
    "df_transformed = df.drop(['backers_count', 'created_at', 'deadline', 'is_starrable', 'launched_at', 'usd_pledged', 'sub_category', 'pledge_per_backer'], axis=1)\n",
    "df_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting dataframe to csv for Spark (later section)\n",
    "#df_transformed.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for colinearity\n",
    "# Set the style of the visualization\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Create a covariance matrix\n",
    "corr = df_transformed.corr()\n",
    "\n",
    "# Generate a mask the size of our covariance matrix\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize = (11,9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are slight skews in some of the feature distributions, but should not affect the machine learning models much. There is also not much colinearity between the given features to be concerned about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dependent variable (success and failure) converted to numerical\n",
    "df_transformed['state'] = df_transformed['state'].replace({'failed': 0, 'successful': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting boolean features to string to include them in one-hot encoding\n",
    "df_transformed['staff_pick'] = df_transformed['staff_pick'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating dummy variables\n",
    "df_transformed = pd.get_dummies(df_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into independent and dependent dataframes\n",
    "X_unscaled = df_transformed.drop('state', axis=1)\n",
    "y = df_transformed.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X_unscaled), columns=list(X_unscaled.columns))\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving forward, since there is some skewness in some of the features, this needs to be addressed by log transforming the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessing skewed distributions\n",
    "cols_to_log = ['creation_to_launch_days', 'name_length', 'usd_goal']\n",
    "df_transformed[cols_to_log].hist(figsize=(8,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 0s with 0.01 and log-transforming\n",
    "for col in cols_to_log:\n",
    "    df_transformed[col] = df_transformed[col].astype('float64').replace(0.0, 0.01)\n",
    "    df_transformed[col] = np.log(df_transformed[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking new distributions\n",
    "df_transformed[cols_to_log].hist(figsize=(8,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unscaled_log = df_transformed.drop('state', axis=1)\n",
    "y_log = df_transformed.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the data\n",
    "scaler = StandardScaler()\n",
    "X_log = pd.DataFrame(scaler.fit_transform(X_unscaled_log), columns=list(X_unscaled_log.columns))\n",
    "X_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_log, y_log, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a logistic regression model (with default parameters)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "y_hat_test = logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression scores\n",
    "print(\"Logistic regression score for training set:\", round(logreg.score(X_train, y_train),5))\n",
    "print(\"Logistic regression score for test set:\", round(logreg.score(X_test, y_test),5))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_hat_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cf(y_true, y_pred, class_names=None, model_name=None):\n",
    "    \"\"\"Plots a confusion matrix\"\"\"\n",
    "    cf = confusion_matrix(y_true, y_pred)\n",
    "    plt.imshow(cf, cmap=plt.cm.Blues)\n",
    "    plt.grid(b=None)\n",
    "    if model_name:\n",
    "        plt.title(\"Confusion Matrix: {}\".format(model_name))\n",
    "    else:\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    class_names = set(y_true)\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    if class_names:\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "    \n",
    "    thresh = cf.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cf.shape[0]), range(cf.shape[1])):\n",
    "        plt.text(j, i, cf[i, j], horizontalalignment='center', color='white' if cf[i, j] > thresh else 'black')\n",
    "\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plot_cf(y_test, y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the AUC-ROC\n",
    "y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "print('AUC:', round(auc(fpr, tpr),5))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model has a fairly good accuracy score of about 71% (test set), with similar results between the test and train set. Looking at the confusion matrix, however, it is somewhat worse at predicting failures compared to successes, and the recall is notably different between the failure and success. \n",
    "\n",
    "The area under the curve (of receiver operating characteristic) is decently high at 0.78, which makes logistic regression not a bad classifier. Later algorithms will try to improve on these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are a large number of features in the dataset, we will attempt to utilize principal component analysis to reduce the dimensionality of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "explained_var = np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the amount of variation explained by PCA with different numbers of components\n",
    "plt.plot(list(range(1, len(explained_var)+1)), explained_var)\n",
    "plt.title('Amount of variation explained by PCA', fontsize=14)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of components explaining 80% of variance:\", np.where(explained_var > 0.8)[0][0])\n",
    "print(\"Number of components explaining 90% of variance:\", np.where(explained_var > 0.9)[0][0])\n",
    "print(\"Number of components explaining 99% of variance:\", np.where(explained_var > 0.99)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evaluating logistic regression model using different number of components\n",
    "n_comps = [58,70,90]\n",
    "for n in n_comps:\n",
    "    pipe = Pipeline([('pca', PCA(n_components=n)), ('clf', LogisticRegression())])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    print(\"\\nNumber of components:\", n)\n",
    "    print(\"Score:\", round(pipe.score(X_test, y_test),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature weightings on each component, in order of average weighting\n",
    "pca = PCA(n_components=90)\n",
    "pca.fit_transform(X)\n",
    "pca_90_components = pd.DataFrame(pca.components_,columns=X.columns).T # Components as columns, features as rows\n",
    "pca_90_components['mean_weight'] = pca_90_components.iloc[:].abs().mean(axis=1)\n",
    "pca_90_components.sort_values('mean_weight', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting feature importances\n",
    "plt.figure(figsize=(20,5))\n",
    "color = cm.CMRmap(np.linspace(0.1,0.8,df.launch_day.nunique()))\n",
    "pca_90_components.mean_weight.sort_values(ascending=False).plot(kind='bar', color=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tables below show the top 10 most important features in the top three most important components.\n",
    "\n",
    "- Component 1 = the top two features relate to the country a project is from, primarily the US and the UK (the top two most common countries).\n",
    "- Component 2 = the top two features relate to whether or not a project was chosen as a staff pick.\n",
    "- Component 3 = the top two features relate to the time of year the project was launched, specifically in October."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_90_components[0].map(lambda x : x).abs().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_90_components[1].map(lambda x : x).abs().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pca_90_components[2].map(lambda x : x).abs().sort_values(ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis requires a large number of components (90) in order to achieve a similar prediction accuracy using plain logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to test multiple different parameters\n",
    "pipe_logreg = Pipeline([('pca', PCA(n_components=90)),\n",
    "                    ('clf', LogisticRegression())])\n",
    "\n",
    "params_logreg = [\n",
    "    {'clf__penalty': ['l1', 'l2'],\n",
    "     'clf__fit_intercept': [True, False],\n",
    "        'clf__C': [0.001, 0.01, 1, 10]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_logreg = GridSearchCV(estimator=pipe_logreg,\n",
    "                  param_grid=params_logreg,\n",
    "                  cv=5)\n",
    "\n",
    "grid_logreg.fit(X_train, y_train)\n",
    "\n",
    "logreg_best_score = grid_logreg.best_score_\n",
    "logreg_best_params = grid_logreg.best_params_\n",
    "\n",
    "print(\"Best accuracy:\", round(logreg_best_score,2))\n",
    "print(\"Best parameters:\", logreg_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results from the logistic regression parameter optimisation:\n",
    "\n",
    "- Best accuracy: 0.71\n",
    "- Best parameters: {'clf__C': 10, 'clf__fit_intercept': True, 'clf__penalty': 'l2'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_best_logreg = Pipeline([('pca', PCA(n_components=90)),\n",
    "                    ('clf', LogisticRegression(C=10, fit_intercept=True, penalty='l2'))])\n",
    "\n",
    "pipe_best_logreg.fit(X_train, y_train)\n",
    "\n",
    "lr_y_hat_train = pipe_best_logreg.predict(X_train)\n",
    "lr_y_hat_test = pipe_best_logreg.predict(X_test)\n",
    "\n",
    "print(\"Logistic regression score for training set:\", round(pipe_best_logreg.score(X_train, y_train),5))\n",
    "print(\"Logistic regression score for test set:\", round(pipe_best_logreg.score(X_test, y_test),5))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, lr_y_hat_test))\n",
    "plot_cf(y_test, lr_y_hat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and hyperparameter optimization using logistic regression, as expected, does not improve on the base logisitic regression model, yielding about the same accuracy. Since PCA does not significantly simplify the machine learning model training process or improve it at all, it will not be used for the following machine learning models and it will also be omitted when training machine learning models using Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using GridSearchCV to test multiple different parameters (can be skipped for tim)\n",
    "rf = RandomForestClassifier(min_samples_split=0.001, verbose=2)\n",
    "\n",
    "params_rf = [ \n",
    "  {'n_estimators': [200, 400],\n",
    "   'max_depth': [20, 35]\n",
    "  }\n",
    "]\n",
    "\n",
    "grid_rf = GridSearchCV(estimator=rf, param_grid=params_rf, cv=5)\n",
    "\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_best_score = grid_rf.best_score_\n",
    "rf_best_params = grid_rf.best_params_\n",
    "\n",
    "print(\"Best accuracy:\", round(rf_best_score,2))\n",
    "print(\"Best parameters:\", rf_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- Best accuracy: 0.74\n",
    "- Best parameters: {'max_depth': 35, 'n_estimators': 400}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf = RandomForestClassifier(max_depth=35, min_samples_split=0.001, n_estimators=400)\n",
    "\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "rf_y_hat_train = best_rf.predict(X_train)\n",
    "rf_y_hat_test = best_rf.predict(X_test)\n",
    "\n",
    "print(\"Random Forest score for training set:\", round(best_rf.score(X_train, y_train),5))\n",
    "print(\"Random Forest score for test set:\", round(best_rf.score(X_test, y_test),5))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, rf_y_hat_test))\n",
    "plot_cf(y_test, rf_y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importance\n",
    "n_features = X_train.shape[1]\n",
    "plt.figure(figsize=(8,20))\n",
    "plt.barh(range(n_features), best_rf.feature_importances_, align='center') \n",
    "plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "plt.title(\"Feature importances in the best Random Forest model\")\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using GridSearchCV to test multiple different parameters\n",
    "\n",
    "xgb = xgb.XGBClassifier(learning_rate=0.1, max_depth=35, verbose=2)\n",
    "\n",
    "params_xgb = [ \n",
    "  {'n_estimators': [100, 200],\n",
    "   'subsample': [0.7, 0.9],\n",
    "   'min_child_weight': [100, 200]\n",
    "  }\n",
    "]\n",
    "\n",
    "grid_xgb = GridSearchCV(estimator=xgb, param_grid=params_xgb, cv=5)\n",
    "\n",
    "grid_xgb.fit(X_train, y_train)\n",
    "\n",
    "xgb_best_score = grid_xgb.best_score_\n",
    "xgb_best_params = grid_xgb.best_params_\n",
    "\n",
    "print(\"Best accuracy:\", round(xgb_best_score,2))\n",
    "print(\"Best parameters:\", xgb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "- Best accuracy: 0.75\n",
    "- Best parameters: {'min_child_weight': 100, 'n_estimators': 100, 'subsample': 0.7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = xgb.XGBClassifier(learning_rate=0.1, max_depth=35, min_child_weight=100, n_estimators=100, subsample=0.7)\n",
    "\n",
    "best_xgb.fit(X_train, y_train)\n",
    "\n",
    "xgb_y_hat_train = best_xgb.predict(X_train)\n",
    "xgb_y_hat_test = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"XGBoost score for training set:\", round(best_xgb.score(X_train, y_train),5))\n",
    "print(\"XGBoost score for test set:\", round(best_xgb.score(X_test, y_test),5))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, xgb_y_hat_test))\n",
    "plot_cf(y_test, xgb_y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting feature importance\n",
    "n_features = X_train.shape[1]\n",
    "plt.figure(figsize=(8,20))\n",
    "plt.barh(range(n_features), best_xgb.feature_importances_, align='center') \n",
    "plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "plt.title(\"Feature importances in the best XGBoost model\")\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above three machine learning models, it appears that the XGBoost algorithm performed the best, yielding an accuracy score of 75.1% for the test set although each of the other algorithms yielded similarly good scores around 70%. Finding the best hyperparameters using GridSearchCV can be very time consuming and does not improve on accuracy significantly, being good for a few percentage points.\n",
    "\n",
    "The feature importance plots are interesting and show how each algorithm weights each feature in classifying it as successful or failed. The XGBoost algorithm heavily weighted staff pick more so than for the random forest algorithm. This feature is important since it highlights the project on the site. The random forest algorithm recognizes this, but also emphasizes the importance of the USD goal in predicting success along with several other features which seem to fit with trends seen in the exploration phase. With PCA, the top components did focus on staff picks, but also on country and time of launch.\n",
    "\n",
    "Furthermore, there might be potential value in doing natural language processing to see how the content of the blurb and name can impact the success of a project in a later analysis more so than just their length. Blurb length is highlighted in the random forest algorithm so it is not an insignificant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "In this section, the same machine learning algorithms above are used in Pyspark and the performance is compared (with gradient boosted trees used in place of xgboost). An auxillary Python script is used to train and evaluate the models using Pyspark and AWS EMR. The job is run in AWS and the results are downloaded (printed in stdout) and included in this notebook. Specifically, the results include area under ROC and a confusion matrix for the test set. Note, a grid search was not used to find the best hyperparameters in training each of the following models like in the models above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aws emr create-cluster --applications Name=Hadoop Name=Hive Name=Pig Name=Hue Name=Spark --ec2-attributes '{\"InstanceProfile\":\"EMR_EC2_DefaultRole\",\"SubnetId\":\"subnet-3d6c6966\",\"EmrManagedSlaveSecurityGroup\":\"sg-0f22d4dbfdcaef582\",\"EmrManagedMasterSecurityGroup\":\"sg-05cfd661629f9ceb4\"}' --release-label emr-5.23.0 --log-uri 's3n://aws-logs-435322005424-us-west-1/elasticmapreduce/' --steps '[{\"Args\":[\"spark-submit\",\"--deploy-mode\",\"cluster\",\"s3://rl-cs696/kickstarter.py\",\"-i\",\"s3://rl-cs696/df.csv\",\"-o\",\"s3://rl-cs696/output\"],\"Type\":\"CUSTOM_JAR\",\"ActionOnFailure\":\"CONTINUE\",\"Jar\":\"command-runner.jar\",\"Properties\":\"\",\"Name\":\"Spark application\"},{\"Args\":[\"spark-submit\",\"--deploy-mode\",\"cluster\",\"s3://rl-cs696/kickstarter.py\",\"-i\",\"s3://rl-cs696/df.csv\",\"-o\",\"s3://rl-cs696/output\"],\"Type\":\"CUSTOM_JAR\",\"ActionOnFailure\":\"CONTINUE\",\"Jar\":\"command-runner.jar\",\"Properties\":\"\",\"Name\":\"Spark application\"}]' --instance-groups '[{\"InstanceCount\":1,\"InstanceGroupType\":\"MASTER\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Master - 1\"},{\"InstanceCount\":2,\"InstanceGroupType\":\"CORE\",\"InstanceType\":\"m3.xlarge\",\"Name\":\"Core - 2\"}]' --auto-scaling-role EMR_AutoScaling_DefaultRole --bootstrap-actions '[{\"Path\":\"s3://rl-cs696/install.sh\",\"Name\":\"Custom action\"}]' --ebs-root-volume-size 10 --service-role EMR_DefaultRole --enable-debugging --name 'kickstarter_v4' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-west-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Logistic Regression, Test Area Under ROC: 0.7535184064953994  \n",
    "Logistic Regression, Confusion Matrix:  \n",
    "[21876,  6450],  \n",
    "[ 9297, 12916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', (21876+12916)/(21876+12916+6450+9297))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forest, Test Area Under ROC: 0.7884483755844446  \n",
    "Random Forest, Confusion Matrix:  \n",
    "[25495,  2831],  \n",
    "[12058, 10155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', (25495+10155)/(25495+10155+2831+12058))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Trees\n",
    "Gradient Boosted Tree, Test Area Under ROC: 0.7956970081050084  \n",
    "Gradient Boosted Tree, Confusion Matrix:  \n",
    "[23599,  4727],  \n",
    "[ 9034, 13179]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', (23599+13179)/(23599+13179+4727+9034))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's machine learning algorithms perform similarly to sklearn's algorithms, with each algorithm yielding about 70% accuracy. The best model in Spark is the gradient boosted trees, which is a general implementation of the XGBoost algorithm, yielding an accuracy score of 72.8%. This is all done using default hyperparameter values and can theoretically be slightly improved using a grid search method as done above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, logisitic regression, random forests, and gradient boosting algorithms were used to classify the success of a given project based on features scraped from the Kickstarter website. Each of the different models performed similarly, yielding around 70% accuracy with the gradient boosting algorithms, in both Sklearn and Spark, slightly edging out with around 75% accuracy.\n",
    "\n",
    "Looking at both trends in the data from our exploration as well as the important features determined by some of the machine learning algorithms, we were able to gain some insight into which features can help a project succeed.\n",
    "\n",
    "Some of the factors that had a **positive effect** on success are:\n",
    "\n",
    "**Most important:**\n",
    "- Smaller project goals\n",
    "- Being chosen as a staff pick\n",
    "- Shorter campaigns\n",
    "- Taking longer between creation and launch\n",
    "- Comics, dance and games projects\n",
    "\n",
    "**Less important:**\n",
    "- Projects from Hong Kong\n",
    "- Film & video and music projects are popular categories on the site, and are fairly successful\n",
    "- Launching on a Tuesday\n",
    "- Launching in October\n",
    "- Launching between 12pm and 2pm UTC\n",
    "- Name and blurb lengths (shorter blurbs and longer names are preferred)\n",
    "\n",
    "Factors which had a **negative effect** on success are:\n",
    "\n",
    "**Most important:**\n",
    "- Large goals\n",
    "- Longer campaigns\n",
    "- Food and journalism projects\n",
    "- Projects from Italy\n",
    "\n",
    "**Less important:**\n",
    "- Launching on a weekend\n",
    "- Launching in July or December\n",
    "- Launching between 6pm and 4am UTC\n",
    "\n",
    "Overall, Kickstarter seems to favor smaller, high-quality projects, particularly comics, dances and games which capture many of the above attributes. It is less suited to larger projects, particularly food and journalism projects. If launching a project on this platform, it will be important to keep these ideas in consideration in order to maximize success, but at the same time, it is good to understand the limitations of machine learning in this context. The data was limited to just the features given on the website, and so there are likely many more factors at play. Also, if more data could be obtained, more insight can be derived. Ultimately, combining business and marketing acumen with data will maximize the success of such crowdfunded projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas/Sklearn Vs Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas and Spark implementations performed similarly to one another in terms of accuracy scores for each machine learning model and in terms of run time and ease of implementation. The dataset, however, was not an especially large dataset, having only 209222 rows. For an even larger dataset, it can be expected that the Spark implementation will run more efficiently than using Pandas.\n",
    "\n",
    "Cleaning the data using Pandas is much easier and interactive, especially when using Jupyter notebooks to visualize the data. It is possible to clean the data using Spark as well, but because Spark jobs are usually batch jobs, it is not as easy to debug code and visualize the data. This is why it was preferred to clean and explore the data using Pandas instead for this project before using Spark. For future steps, it will be interesting to consider Python's parallelizing dataframe libraries such as Dask and Vaex to clean and visualize large datasets. These libraries are built on top of Pandas and can be used within Jupyter, which will make the process easier and more versatile than sending jobs to AWS EMR. If these libraries can process data as fast as Spark can, this may be the preferred way for big data processing and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
